{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jay Mahakal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas matplotlib seaborn scipy numpy statsmodels scikit-learn prince plotly networkx matplotlib_venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "import prince\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4660aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_01 = pd.read_csv(\"Urgent - Responses on Survey on AI Use and Adoption Shared.csv\",header=0,encoding=\"cp1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433eb61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_01.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b79e88",
   "metadata": {},
   "source": [
    "# Assess the Association Between Entity Type and AI Adoption Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966eafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_01[[\"Type of Entity\", \"What is the current status of AI adoption in your organization?\"]].copy()\n",
    "df.columns = [\"EntityType\", \"AIStatus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00087c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5af90",
   "metadata": {},
   "source": [
    "Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(df[\"EntityType\"], df[\"AIStatus\"])\n",
    "print(\"Contingency Table:\\n\", crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437cc839",
   "metadata": {},
   "source": [
    "## Cramer's V Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752512a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = crosstab.sum().sum()\n",
    "phi2 = chi2 / n\n",
    "r, k = crosstab.shape\n",
    "cramers_v = np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "print(f\"Cramer's V: {cramers_v:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb71081",
   "metadata": {},
   "source": [
    "## Heatmap of Raw Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49670a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(\n",
    "    crosstab,\n",
    "    text_auto=True,  \n",
    "    color_continuous_scale='Blues',\n",
    "    labels=dict(x=\"AI Adoption Status\", y=\"Entity Type\", color=\"Count\"),\n",
    "    title=\"Heatmap: Entity Type vs AI Adoption Status\",\n",
    "    aspect=\"auto\" \n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"AI Adoption Status\",\n",
    "    yaxis_title=\"Entity Type\",\n",
    "    font=dict(size=14),\n",
    "    title_font=dict(size=18),\n",
    "    margin=dict(l=80, r=80, t=100, b=80)\n",
    ")\n",
    "\n",
    "fig.update_coloraxes(colorbar=dict(title=\"Count\"))\n",
    "fig.update_traces(hovertemplate=\"Entity Type: %{y}<br>AI Adoption Status: %{x}<br>Count: %{z}<extra></extra>\")\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6250f",
   "metadata": {},
   "source": [
    "## Count Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68930947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_01 = crosstab.reset_index().melt(id_vars=crosstab.index.name, var_name='AI Adoption Status', value_name='Count')\n",
    "\n",
    "fig = px.bar(\n",
    "    df_01,\n",
    "    x='EntityType',\n",
    "    y='Count',\n",
    "    color='AI Adoption Status',\n",
    "    barmode='group', \n",
    "    title='AI Adoption by Entity Type',\n",
    "    labels={'Count': 'Count', 'Entity Type': 'Entity Type', 'AI Adoption Status': 'AI Adoption Status'},\n",
    "    height=500,\n",
    "    width=900,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    font=dict(size=14),\n",
    "    title_font=dict(size=18),\n",
    "    margin=dict(l=80, r=50, t=80, b=120)\n",
    ")\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39cbf3a",
   "metadata": {},
   "source": [
    "## Proportional Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46559d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = crosstab.div(crosstab.sum(axis=1), axis=0)\n",
    "df_prop = proportions.reset_index().melt(id_vars=crosstab.index.name or 'Entity Type',\n",
    "                                        var_name='AI Adoption Status',\n",
    "                                        value_name='Proportion')\n",
    "\n",
    "fig = px.bar(\n",
    "    df_prop,\n",
    "    x='EntityType',\n",
    "    y='Proportion',\n",
    "    color='AI Adoption Status',\n",
    "    title='Proportions of AI Adoption by Entity Type',\n",
    "    labels={'Proportion': 'Proportion', 'Entity Type': 'Entity Type', 'AI Adoption Status': 'AI Adoption Status'},\n",
    "    barmode='stack',\n",
    "    color_continuous_scale=px.colors.sequential.Viridis,\n",
    "    height=500,\n",
    "    width=900\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    yaxis=dict(tickformat=\".0%\"), \n",
    "    font=dict(size=14),\n",
    "    title_font=dict(size=18),\n",
    "    margin=dict(l=80, r=50, t=80, b=120)\n",
    ")\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230ff4c",
   "metadata": {},
   "source": [
    "##  Mosaic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(\n",
    "    df,\n",
    "    path=['EntityType', 'AIStatus'],  \n",
    "    title=\"Treemap: Entity Type vs AI Adoption Status\",\n",
    "    values=None,  \n",
    "    color='AIStatus',  \n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(t=50, l=25, r=25, b=25),\n",
    "    title_font=dict(size=18)\n",
    ")\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98440aba",
   "metadata": {},
   "source": [
    "## Correspondence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = prince.CA(n_components=2, random_state=42).fit(crosstab)\n",
    "row_coords = ca.row_coordinates(crosstab)\n",
    "col_coords = ca.column_coordinates(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14873c4b",
   "metadata": {},
   "source": [
    "## Visualize Correspondence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6400741",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=row_coords.iloc[:, 0],\n",
    "    y=row_coords.iloc[:, 1],\n",
    "    mode='markers+text',\n",
    "    name='Entity Type',\n",
    "    text=row_coords.index,\n",
    "    textposition='top center',\n",
    "    marker=dict(color='blue', size=10),\n",
    "    textfont=dict(color='blue')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=col_coords.iloc[:, 0],\n",
    "    y=col_coords.iloc[:, 1],\n",
    "    mode='markers+text',\n",
    "    name='AI Status',\n",
    "    text=col_coords.index,\n",
    "    textposition='top center',\n",
    "    marker=dict(color='red', size=10),\n",
    "    textfont=dict(color='red')\n",
    "))\n",
    "\n",
    "fig.add_shape(type=\"line\", x0=min(row_coords.iloc[:, 0].min(), col_coords.iloc[:, 0].min()),\n",
    "                      y0=0, x1=max(row_coords.iloc[:, 0].max(), col_coords.iloc[:, 0].max()), y1=0,\n",
    "              line=dict(color=\"grey\", width=1))\n",
    "fig.add_shape(type=\"line\", x0=0, y0=min(row_coords.iloc[:, 1].min(), col_coords.iloc[:, 1].min()),\n",
    "                      x1=0, y1=max(row_coords.iloc[:, 1].max(), col_coords.iloc[:, 1].max()),\n",
    "              line=dict(color=\"grey\", width=1))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Correspondence Analysis\",\n",
    "    xaxis_title=\"Dimension 1\",\n",
    "    yaxis_title=\"Dimension 2\",\n",
    "    xaxis=dict(zeroline=False),\n",
    "    yaxis=dict(zeroline=False),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    legend=dict(x=0.85, y=0.95),\n",
    "    template=\"plotly_white\",\n",
    "    dragmode=\"pan\"\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f94526",
   "metadata": {},
   "source": [
    "# Evaluate Differences in AI Maturity Levels Across Drivers of Adoption --- Not Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_01[[\"How would you categorize your current AI usage maturity level?\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_01[[\"What are the primary drivers for adopting AI in your organization? (Select all that apply)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_01[[\"How would you categorize your current AI usage maturity level?\",\n",
    "\"What are the primary drivers for adopting AI in your organization? (Select all that apply)\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef172ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"AIMaturity\", \"Drivers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"AIMaturity\", \"Drivers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6392a6e0",
   "metadata": {},
   "source": [
    "## Transform Multi-Select Drivers → Binary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_dummies_01 = df[\"Drivers\"].str.get_dummies(sep=';')\n",
    "driver_dummies_02 = df[\"AIMaturity\"].str.get_dummies(sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e244d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded = pd.concat([driver_dummies_01, driver_dummies_02], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExpanded Data (first rows):\\n\", df_expanded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f52ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for driver in driver_dummies.columns:\n",
    "    crosstab = pd.crosstab(df_expanded[\"AIMaturity\"], df_expanded[driver])\n",
    "    chi2, p, dof, expected = chi2_contingency(crosstab)\n",
    "    print(f\"\\nDriver: {driver}\")\n",
    "    print(\"Chi-square Statistic:\", chi2, \" p-value:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2/min(k-1, r-1))\n",
    "\n",
    "\n",
    "def interpret_cramers_v(value):\n",
    "    if value < 0.1:\n",
    "        return \"Negligible\"\n",
    "    elif value < 0.3:\n",
    "        return \"Small\"\n",
    "    elif value < 0.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "\n",
    "for driver in driver_dummies.columns:\n",
    "    crosstab = pd.crosstab(df_expanded[\"AIMaturity\"], df_expanded[driver])\n",
    "    cv = cramers_v(crosstab)\n",
    "    interpretation = interpret_cramers_v(cv)\n",
    "    print(f\"Cramer's V for {driver}: {round(cv, 3)} ({interpretation} association)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"AIMaturity ~ \" + \" + \".join(driver_dummies.columns)\n",
    "model = smf.mnlogit(formula, data=df_expanded)\n",
    "result = model.fit(method='newton', maxiter=100, disp=False)\n",
    "print(\"\\nMultinomial Logistic Regression Summary:\\n\", result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f748d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_all = pd.crosstab(df_expanded[\"AIMaturity\"], df_expanded[driver_dummies.columns].idxmax(axis=1))\n",
    "ca = prince.CA(n_components=2, random_state=42)\n",
    "ca = ca.fit(crosstab_all)\n",
    "ca.plot_coordinates(X=crosstab_all, figsize=(8,6), show_row_labels=True, show_col_labels=True)\n",
    "plt.title(\"Correspondence Analysis: AI Maturity vs Drivers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "prop_table = df_expanded.groupby(\"AIMaturity\")[driver_dummies.columns].mean()\n",
    "sns.heatmap(prop_table, annot=True, cmap=\"YlGnBu\")\n",
    "plt.title(\"Proportion of Entities Selecting Each Driver by AI Maturity\")\n",
    "plt.xlabel(\"Drivers\")\n",
    "plt.ylabel(\"AI Maturity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c56b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_table.T.plot(kind='bar', stacked=True, figsize=(12,6), colormap=\"viridis\")\n",
    "plt.title(\"Driver Distribution Across AI Maturity Levels\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e85175",
   "metadata": {},
   "source": [
    "# Investigate the Relationship Between Key Challenges and Risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc629110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenges = data_01[\"What are the key challenges your organization faces in implementing AI? (Select all that apply)\\n\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565194e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risks = data_01[\"What risks do you associate with AI adoption? (Select all that apply)\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d99429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_matrix(series, prefix):\n",
    "    series = series.fillna(\"\").astype(str).apply(lambda x: [s.strip() for s in x.split(\";\") if s.strip()])\n",
    "    uniq = sorted(set(v for lst in series for v in lst))\n",
    "    out = pd.DataFrame(index=series.index)\n",
    "    for u in uniq:\n",
    "        colname = f\"{prefix}__{u.replace(' ', '_')}\"\n",
    "        out[colname] = series.apply(lambda lst: int(u in lst))\n",
    "    return out\n",
    "\n",
    "challenges_bin = indicator_matrix(df_challenges, \"Challenge\")\n",
    "risks_bin = indicator_matrix(df_risks, \"Risk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c719c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(challenges_bin.head())\n",
    "print(risks_bin.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8063e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_mat = pd.DataFrame(index=challenges_bin.columns, columns=risks_bin.columns, dtype=float)\n",
    "\n",
    "for c in challenges_bin.columns:\n",
    "    for r in risks_bin.columns:\n",
    "        tab = pd.crosstab(challenges_bin[c], risks_bin[r])\n",
    "        if tab.shape == (2,2):\n",
    "            chi2, p, dof, exp = chi2_contingency(tab)\n",
    "            n = tab.values.sum()\n",
    "            phi = np.sqrt(chi2/n)\n",
    "            phi_mat.loc[c,r] = phi\n",
    "\n",
    "# Heatmap of phi coefficients\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(phi_mat.astype(float), cmap=\"coolwarm\", center=0, annot=True, fmt=\".2f\")\n",
    "plt.title(\"Phi Correlation Matrix: Challenges vs Risks\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_col = 'Challenge__Data_privacy_and_cybersecurity_concerns'\n",
    "risk_col = 'Risk__Data_security_and_privacy_breaches'\n",
    "\n",
    "tab = pd.crosstab(challenges_bin[challenge_col], risks_bin[risk_col])\n",
    "result = mcnemar(tab, exact=True)\n",
    "\n",
    "print(\"McNemar’s test\")\n",
    "print(\"Table:\\n\", tab)\n",
    "print(\"statistic:\", result.statistic, \"p-value:\", result.pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674333e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2\n",
    "\n",
    "a = challenges_bin[challenge_col].sum()       # challenge selected\n",
    "b = risks_bin[risk_col].sum()                 # risk selected\n",
    "ab = ((challenges_bin[challenge_col]==1) & (risks_bin[risk_col]==1)).sum()\n",
    "\n",
    "venn2(subsets=(a-ab, b-ab, ab), set_labels=(challenge_col, risk_col))\n",
    "plt.title(\"Overlap between Challenge and Risk\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "jac_mat = pd.DataFrame(index=challenges_bin.columns, columns=risks_bin.columns, dtype=float)\n",
    "\n",
    "for c in challenges_bin.columns:\n",
    "    for r in risks_bin.columns:\n",
    "        jac_mat.loc[c, r] = jaccard_score(challenges_bin[c], risks_bin[r])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(jac_mat.astype(float), cmap=\"Greens\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Jaccard Similarity Matrix: Challenges vs Risks\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c276491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph based on Phi correlations (or Jaccard)\n",
    "G = nx.Graph()\n",
    "\n",
    "# add nodes separately for clarity\n",
    "for c in challenges_bin.columns:\n",
    "    G.add_node(c, type=\"Challenge\")\n",
    "for r in risks_bin.columns:\n",
    "    G.add_node(r, type=\"Risk\")\n",
    "\n",
    "# add edges when association is strong\n",
    "for c in challenges_bin.columns:\n",
    "    for r in risks_bin.columns:\n",
    "        phi = phi_mat.loc[c, r]\n",
    "        if pd.notna(phi) and phi > 0.25:   # threshold for clarity\n",
    "            G.add_edge(c, r, weight=phi)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "pos = nx.spring_layout(G, k=0.4, seed=42)\n",
    "edge_weights = [d[\"weight\"]*5 for (_,_,d) in G.edges(data=True)]  # scale for visibility\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos,\n",
    "                       node_color=[\"lightblue\" if G.nodes[n][\"type\"]==\"Challenge\" else \"lightcoral\"\n",
    "                                   for n in G.nodes],\n",
    "                       node_size=800)\n",
    "nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.6)\n",
    "nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "\n",
    "plt.title(\"Challenge–Risk Network Graph (edges = Phi strength > 0.25)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd955e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Merge challenges + risks into one binary feature space\n",
    "combined = pd.concat([challenges_bin, risks_bin], axis=1)\n",
    "\n",
    "# Hierarchical clustering\n",
    "Z = linkage(combined.T, method='ward')  # cluster on transposed matrix (features)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "dendrogram(Z, labels=combined.columns, leaf_rotation=90)\n",
    "plt.title(\"Hierarchical Clustering of Challenges and Risks\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "\n",
    "mca = prince.MCA(n_components=2, random_state=42)\n",
    "mca = mca.fit(combined)\n",
    "\n",
    "coords = mca.column_coordinates(combined)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(coords[0], coords[1])\n",
    "\n",
    "for i, txt in enumerate(coords.index):\n",
    "    plt.annotate(txt, (coords.iloc[i,0], coords.iloc[i,1]), fontsize=8)\n",
    "\n",
    "plt.title(\"MCA: Challenges & Risks in 2D\")\n",
    "plt.axhline(0, color='gray', lw=0.5)\n",
    "plt.axvline(0, color='gray', lw=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize\n",
    "X = StandardScaler().fit_transform(combined)\n",
    "\n",
    "# Fit KMeans (try k=3, adjust as needed)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "data_01[\"Cluster\"] = labels\n",
    "\n",
    "print(data_01.groupby(\"Cluster\").size())\n",
    "\n",
    "# Visualize clusters on MCA 2D projection\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(coords[0], coords[1], c=['red','blue','green']*10)  # naive coloring, adjust to labels\n",
    "plt.title(\"Clusters of Organizations by Challenges/Risks\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a480d",
   "metadata": {},
   "source": [
    "# Compare Proportions of AI Trends Exploration by Maturity Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_01[[\"Which emerging AI trends is your organization exploring? (Select all that apply)\", \"How would you categorize your current AI usage maturity level?\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"Trends\", \"AIMaturity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a41c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse your indicator function\n",
    "trends_bin = indicator_matrix(df[\"Trends\"], prefix=\"Trend\")\n",
    "\n",
    "# Attach maturity\n",
    "trends_bin[\"AIMaturity\"] = df[\"AIMaturity\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute proportion of each trend within each maturity group\n",
    "trend_props = trends_bin.groupby(\"AIMaturity\").mean().T\n",
    "print(trend_props.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd576ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "results = []\n",
    "levels = trends_bin[\"AIMaturity\"].unique()\n",
    "\n",
    "for trend in [c for c in trends_bin.columns if c.startswith(\"Trend__\")]:\n",
    "    for i in range(len(levels)):\n",
    "        for j in range(i+1, len(levels)):\n",
    "            g1, g2 = levels[i], levels[j]\n",
    "            x1 = trends_bin.loc[trends_bin[\"AIMaturity\"]==g1, trend].sum()\n",
    "            n1 = (trends_bin[\"AIMaturity\"]==g1).sum()\n",
    "            x2 = trends_bin.loc[trends_bin[\"AIMaturity\"]==g2, trend].sum()\n",
    "            n2 = (trends_bin[\"AIMaturity\"]==g2).sum()\n",
    "\n",
    "            stat, pval = proportions_ztest([x1,x2], [n1,n2])\n",
    "            results.append((trend, g1, g2, stat, pval))\n",
    "\n",
    "# Adjust for multiple comparisons (Bonferroni)\n",
    "df_results = pd.DataFrame(results, columns=[\"Trend\",\"Group1\",\"Group2\",\"Z\",\"pval\"])\n",
    "df_results[\"pval_adj\"] = multipletests(df_results[\"pval\"], method=\"bonferroni\")[1]\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trend in [c for c in trends_bin.columns if c.startswith(\"Trend__\")]:\n",
    "    tab = pd.crosstab(trends_bin[\"AIMaturity\"], trends_bin[trend])\n",
    "    chi2, p, dof, exp = chi2_contingency(tab)\n",
    "    if p < 0.05:\n",
    "        print(f\"{trend}: chi2={chi2:.2f}, p={p:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0956d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=trends_bin.melt(id_vars=\"AIMaturity\", var_name=\"Trend\", value_name=\"Selected\"),\n",
    "            x=\"Trend\", y=\"Selected\", hue=\"AIMaturity\", ci=95)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Proportion selected\")\n",
    "plt.title(\"AI Trend Exploration by Maturity Level\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49719f9c",
   "metadata": {},
   "source": [
    "# Governance Frameworks → Audit Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daacae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any framework selected = 1, else 0\n",
    "frameworks_any = data_01[\"What governance frameworks are in place for AI? (Select all that apply)\"].fillna(\"\").apply(lambda x: int(len(str(x).strip()) > 0))\n",
    "audits = data_01[\"Do you have an established process to audit and review AI models regularly for fairness, bias, and ethical concerns?\"].fillna(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse audit answers to Yes vs Not-Yes (you can refine)\n",
    "audit_yes = audits.apply(lambda x: 1 if \"Yes\" in x else 0)\n",
    "\n",
    "tab = pd.crosstab(frameworks_any, audit_yes)\n",
    "print(\"Contingency Table:\\n\", tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b248dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "oddsratio, pval = fisher_exact(tab)\n",
    "print(f\"Fisher’s Exact Test: OR={oddsratio:.2f}, p={pval:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "mosaic_data = {(f\"Frameworks={i}\", f\"Audit={j}\"): tab.loc[i,j] for i in tab.index for j in tab.columns}\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "mosaic(mosaic_data, title=\"Governance Frameworks vs Audit Process\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    \"Frameworks\": frameworks_any.map({0:\"No Frameworks\", 1:\"Frameworks\"}),\n",
    "    \"Audit\": audits\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(data=df_plot, x=\"Audit\", hue=\"Frameworks\")\n",
    "plt.title(\"Audit Process by Governance Framework Presence\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_matrix(series, prefix):\n",
    "    series = series.fillna(\"\").astype(str).apply(lambda x: [s.strip() for s in x.split(\";\") if s.strip()])\n",
    "    uniq = sorted(set(v for lst in series for v in lst))\n",
    "    out = pd.DataFrame(index=series.index)\n",
    "    for u in uniq:\n",
    "        colname = f\"{prefix}__{u.replace(' ', '_')}\"\n",
    "        out[colname] = series.apply(lambda lst: int(u in lst))\n",
    "    return out\n",
    "\n",
    "frameworks_bin = indicator_matrix(data_01[\"What governance frameworks are in place for AI? (Select all that apply)\"], \"Framework\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_yes = audits.apply(lambda x: 1 if \"Yes\" in str(x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "results = []\n",
    "for fw in frameworks_bin.columns:\n",
    "    tab = pd.crosstab(frameworks_bin[fw], audit_yes)\n",
    "    if tab.shape == (2,2):\n",
    "        or_val, pval = fisher_exact(tab)\n",
    "        results.append((fw, tab.iloc[1,1], tab.iloc[1,0], or_val, pval))\n",
    "\n",
    "df_fw_results = pd.DataFrame(results, columns=[\"Framework\", \"Audit_Yes\", \"Audit_No\", \"OddsRatio\", \"pval\"])\n",
    "print(df_fw_results.sort_values(\"pval\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=df_fw_results, x=\"OddsRatio\", y=\"Framework\", hue=(df_fw_results[\"pval\"]<0.05))\n",
    "plt.axvline(1, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Odds Ratios: Governance Framework vs Audit (Yes)\")\n",
    "plt.xlabel(\"Odds Ratio (Audit=Yes)\")\n",
    "plt.ylabel(\"\")\n",
    "plt.legend(title=\"Significant (p<0.05)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3950a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "results = []\n",
    "for fw in frameworks_bin.columns:\n",
    "    tab = pd.crosstab(frameworks_bin[fw], audit_yes)\n",
    "    if tab.shape == (2,2):\n",
    "        or_val, pval = fisher_exact(tab)\n",
    "        results.append((fw, tab.iloc[1,1], tab.iloc[1,0], or_val, pval))\n",
    "\n",
    "df_fw_results = pd.DataFrame(results, \n",
    "    columns=[\"Framework\", \"Audit_Yes\", \"Audit_No\", \"OddsRatio\", \"pval\"])\n",
    "\n",
    "# --- Adjust for multiple tests ---\n",
    "df_fw_results[\"pval_bonf\"] = multipletests(df_fw_results[\"pval\"], method=\"bonferroni\")[1]\n",
    "df_fw_results[\"pval_fdr\"]  = multipletests(df_fw_results[\"pval\"], method=\"fdr_bh\")[1]\n",
    "\n",
    "print(df_fw_results.sort_values(\"pval\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=df_fw_results, \n",
    "            x=\"OddsRatio\", y=\"Framework\", \n",
    "            hue=(df_fw_results[\"pval_fdr\"]<0.05))\n",
    "plt.axvline(1, color=\"red\", linestyle=\"--\")\n",
    "plt.title(\"Odds Ratios: Governance Framework vs Audit (Yes)\")\n",
    "plt.xlabel(\"Odds Ratio (Audit=Yes)\")\n",
    "plt.ylabel(\"\")\n",
    "plt.legend(title=\"Significant (FDR<0.05)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18975fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_status = data_01[\"Do you have an established process to audit and review AI models regularly for fairness, bias, and ethical concerns?\"].fillna(\"No\").replace({\n",
    "    \"Yes\": \"Yes\",\n",
    "    \"In Progress\": \"In Progress\",\n",
    "    \"Not evaluated as yet\": \"No\",\n",
    "    \"No\": \"No\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58123cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d550833",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_multi = pd.crosstab(frameworks_any, audit_status)\n",
    "print(\"Contingency Table:\\n\", tab_multi)\n",
    "\n",
    "chi2, p, dof, exp = chi2_contingency(tab_multi)\n",
    "print(f\"Chi-square test: chi2={chi2:.2f}, p={p:.4f}, dof={dof}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df_reg = pd.DataFrame({\n",
    "    \"Frameworks\": frameworks_any,\n",
    "    \"Audit\": audit_status\n",
    "})\n",
    "\n",
    "# Encode Audit as categorical with \"No\" as baseline\n",
    "y = pd.Categorical(df_reg[\"Audit\"], categories=[\"No\", \"In Progress\", \"Yes\"])\n",
    "X = sm.add_constant(df_reg[\"Frameworks\"])\n",
    "\n",
    "model = sm.MNLogit(y.codes, X).fit(disp=0)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.catplot(data=df_reg, x=\"Audit\", hue=\"Frameworks\", kind=\"count\", height=6, aspect=1.3)\n",
    "plt.title(\"Audit Status by Governance Framework Presence\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frameworks_bin = indicator_matrix(data_01[\"What governance frameworks are in place for AI? (Select all that apply)\"], prefix=\"Framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324339fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "results = []\n",
    "\n",
    "for fw in frameworks_bin.columns:\n",
    "    df_reg = pd.DataFrame({\n",
    "        \"Framework\": frameworks_bin[fw],\n",
    "        \"Audit\": audit_status\n",
    "    })\n",
    "    # Encode Audit as categorical with \"No\" as baseline\n",
    "    y = pd.Categorical(df_reg[\"Audit\"], categories=[\"No\", \"In Progress\", \"Yes\"])\n",
    "    X = sm.add_constant(df_reg[\"Framework\"])\n",
    "    \n",
    "    try:\n",
    "        model = sm.MNLogit(y.codes, X).fit(disp=0)\n",
    "        params = model.params.loc[1]  # coefficients for \"Framework=1\"\n",
    "        conf = model.conf_int().loc[1]\n",
    "        odds = np.exp(params)\n",
    "        odds_ci = np.exp(conf)\n",
    "        \n",
    "        results.append({\n",
    "            \"Framework\": fw,\n",
    "            \"Category\": \"In Progress\",\n",
    "            \"OR\": odds[0], \"CI_low\": odds_ci[0][0], \"CI_high\": odds_ci[0][1], \"pval\": model.pvalues.loc[1][0]\n",
    "        })\n",
    "        results.append({\n",
    "            \"Framework\": fw,\n",
    "            \"Category\": \"Yes\",\n",
    "            \"OR\": odds[1], \"CI_low\": odds_ci[1][0], \"CI_high\": odds_ci[1][1], \"pval\": model.pvalues.loc[1][1]\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {fw} due to error: {e}\")\n",
    "\n",
    "df_mnlogit = pd.DataFrame(results)\n",
    "print(df_mnlogit.sort_values(\"pval\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "df_mnlogit[\"pval_fdr\"] = multipletests(df_mnlogit[\"pval\"], method=\"fdr_bh\")[1]\n",
    "df_mnlogit[\"pval_bonf\"] = multipletests(df_mnlogit[\"pval\"], method=\"bonferroni\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82041040",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=df_mnlogit, x=\"OR\", y=\"Framework\", hue=\"Category\", palette=\"Set1\")\n",
    "plt.axvline(1, color=\"black\", linestyle=\"--\")\n",
    "plt.title(\"Odds Ratios: Frameworks vs Audit Outcomes\")\n",
    "plt.xlabel(\"Odds Ratio (relative to 'No')\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243ee1b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73acb856",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48095da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from scipy.stats import chi2_contingency, fisher_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff201b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_01 = pd.read_csv(\"Urgent - Responses on Survey on AI Use and Adoption Shared.csv\",header=0,encoding=\"cp1252\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08745fca",
   "metadata": {},
   "source": [
    "## Reseach Question : Assess the Association Between Entity Type and AI Adoption Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caff75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_01[[\n",
    "    \"Type of Entity\",\n",
    "    \"What is the current status of AI adoption in your organization?\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9aa885",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"entity_type\", \"ai_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(df[\"entity_type\"], df[\"ai_status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9741b46",
   "metadata": {},
   "source": [
    "**Cramér's V (effect size)**\n",
    "\n",
    "Cramér's V is a statistical measure of association or effect size between two categorical variables. It gives you an idea of how strongly the two variables are related in a contingency table.\n",
    "\n",
    "Reason for Choosing:\n",
    "1. it's independent of sample size and we have less sample size.\n",
    "2. A value between 0 and 1 (effect size)\n",
    "3. You want to know the practical importance of the association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = contingency_table.sum().sum()\n",
    "phi2 = chi2 / n\n",
    "r, k = contingency_table.shape\n",
    "cramers_v = np.sqrt(phi2 / min(k - 1, r - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cramér's V = {cramers_v:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800aed3b",
   "metadata": {},
   "source": [
    "Result : 0.402 indicates a moderately strong association between the two categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0774d2",
   "metadata": {},
   "source": [
    "## Heatmap of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(\n",
    "    contingency_table,\n",
    "    text_auto=True,\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    labels=dict(x=\"AI Status\", y=\"Entity Type\", color=\"Count\"),\n",
    "    title=\"Entity Type vs AI Adoption Status (Counts)\"\n",
    ")\n",
    "fig.show(renderer=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c82da",
   "metadata": {},
   "source": [
    "## Proportional Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "proportions_reset = proportions.reset_index().melt(\n",
    "    id_vars=\"entity_type\", var_name=\"AI Status\", value_name=\"Proportion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc51aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(\n",
    "    proportions_reset,\n",
    "    x=\"entity_type\",\n",
    "    y=\"Proportion\",\n",
    "    color=\"AI Status\",\n",
    "    barmode=\"group\",  # or \"stack\" if you want stacked bars\n",
    "    text=proportions_reset[\"Proportion\"].apply(lambda x: f\"{x:.0%}\")\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Proportion of AI Adoption Status by Entity Type\",\n",
    "    xaxis_title=\"Entity Type\",\n",
    "    yaxis_title=\"Proportion\",\n",
    "    xaxis_tickangle=-30,\n",
    "    template=\"plotly_white\",  # Use a clean theme\n",
    "    legend_title=\"AI Status\",\n",
    "    height=600,\n",
    "    width=900,\n",
    ")\n",
    "\n",
    "fig.update_yaxes(tickformat=\".0%\", range=[0, 1])\n",
    "fig.update_traces(textposition='outside')  # Show % above bars\n",
    "fig.show(renderer=\"notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4844287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd090079",
   "metadata": {},
   "source": [
    "# Reseach Question : Evaluate Differences in AI Maturity Levels Across Drivers of Adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339308f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_01[[\n",
    "    \"How would you categorize your current AI usage maturity level?\",\n",
    "    \"What are the primary drivers for adopting AI in your organization? (Select all that apply)\"\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"ai_maturity\", \"drivers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b25ca",
   "metadata": {},
   "source": [
    "*multi-select drivers into binary indicator columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drivers = sorted(\n",
    "    set(\n",
    "        d.strip()\n",
    "        for entry in df[\"drivers\"].dropna()\n",
    "        for d in str(entry).split(\";\")\n",
    "        if d.strip() != \"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "for driver in all_drivers:\n",
    "    df[driver] = df[\"drivers\"].apply(\n",
    "        lambda x: 1 if driver in str(x) else 0\n",
    "    )\n",
    "\n",
    "print(\"Drivers expanded into binary columns:\", all_drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "cramers_results = []\n",
    "\n",
    "for driver in all_drivers:\n",
    "    contingency = pd.crosstab(df[\"ai_maturity\"], df[driver])\n",
    "    chi2, _, _, _ = chi2_contingency(contingency)\n",
    "    n = contingency.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = contingency.shape\n",
    "    cramers_v = np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "    if cramers_v < 0.1:\n",
    "        interpretation = \"Negligible\"\n",
    "    elif cramers_v < 0.3:\n",
    "        interpretation = \"Small\"\n",
    "    elif cramers_v < 0.5:\n",
    "        interpretation = \"Medium\"\n",
    "    else:\n",
    "        interpretation = \"Large\"\n",
    "\n",
    "    cramers_results.append({\n",
    "        \"Driver\": driver,\n",
    "        \"Cramers_V\": round(cramers_v, 3),\n",
    "        \"Effect\": interpretation\n",
    "    })\n",
    "\n",
    "cramers_df = pd.DataFrame(cramers_results).sort_values(\"Cramers_V\", ascending=False)\n",
    "print(\"\\nCramér’s V results:\\n\", cramers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_means = df.groupby(\"ai_maturity\")[all_drivers].mean().round(2).reset_index()\n",
    "driver_means_melted = driver_means.melt(id_vars=\"ai_maturity\", var_name=\"Driver\", value_name=\"Proportion\")\n",
    "fig = px.imshow(\n",
    "    driver_means.set_index(\"ai_maturity\").T,\n",
    "    text_auto=True,\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"YlGnBu\",\n",
    "    labels=dict(x=\"AI Maturity Level\", y=\"Driver\", color=\"Proportion\")\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Proportion of Organizations Selecting Each Driver by AI Maturity\",\n",
    "    xaxis_title=\"AI Maturity Level\",\n",
    "    yaxis_title=\"Driver\",\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    font=dict(size=12),\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b931a",
   "metadata": {},
   "source": [
    "# Research Question: Investigate the Relationship Between Key Challenges and Risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "514fdd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_01[[\n",
    "    \"What risks do you associate with AI adoption? (Select all that apply)\",\n",
    "    'What are the key challenges your organization faces in implementing AI? (Select all that apply)\\n'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3837c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"risks\", \"challenges\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e7cd0",
   "metadata": {},
   "source": [
    "*multi-select drivers into binary indicator columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f3e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_risks = sorted(\n",
    "    set(\n",
    "        r.strip()\n",
    "        for entry in df[\"risks\"].dropna()\n",
    "        for r in str(entry).split(\";\")\n",
    "        if r.strip() != \"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "065380cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_challenges = sorted(\n",
    "    set(\n",
    "        c.strip()\n",
    "        for entry in df[\"challenges\"].dropna()\n",
    "        for c in str(entry).split(\";\")\n",
    "        if c.strip() != \"\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea6f897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n",
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\2841333931.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[risk] = df[\"risks\"].apply(\n"
     ]
    }
   ],
   "source": [
    "for risk in all_risks:\n",
    "    df[risk] = df[\"risks\"].apply(\n",
    "        lambda x: 1 if risk in str(x) else 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4518f6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IFSCA\\AppData\\Local\\Temp\\ipykernel_15592\\782776217.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[challenge] = df[\"challenges\"].apply(\n"
     ]
    }
   ],
   "source": [
    "for challenge in all_challenges:\n",
    "    df[challenge] = df[\"challenges\"].apply(\n",
    "        lambda x: 1 if challenge in str(x) else 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae40ef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Risks: ['Bias and fairness issues', 'Cyber security', 'Data security and privacy breaches', 'Ethical and societal concerns', 'Lack of explainability in AI models', 'Lack of reliability on AI', 'NA', 'NOT APPLICABLE', 'Not Applicable', 'Not evaluated as yet', 'Not yet adapted but under consideration', 'Over-reliance on automation', 'Reliance on past data to predict the future', 'Successful implementaton and usage', 'We are not using AI', 'hallucinations', 'very early stage, will come to know gradually']\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracted Risks:\", all_risks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dda0951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Challenges: ['AI Not being used', 'Data privacy and cybersecurity concerns', 'Insufficient resources and funding', 'Interoperability with legacy systems', \"It's at the initial stage at Global level\", 'Lack of reliability on AI', 'Lack of skilled talent', 'NA', 'NOT APPLICABLE', 'None', 'Not Applicable', 'Not evaluated as yet', 'Not yet adapted but under consideration', 'Rapid technological changes', 'Still to determine use case', 'We are not using AI', 'not applicable', 'organization bureaucracy', 'very early stage, will come to know gradually']\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracted Challenges:\", all_challenges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c33a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for risk in all_risks:\n",
    "    for challenge in all_challenges:\n",
    "        contingency = pd.crosstab(df[risk], df[challenge])\n",
    "        if contingency.shape == (2, 2): \n",
    "            oddsratio, p_value = fisher_exact(contingency)\n",
    "            results.append({\n",
    "                \"Risk\": risk,\n",
    "                \"Challenge\": challenge,\n",
    "                \"OddsRatio\": round(oddsratio, 2),\n",
    "                \"p_value\": round(p_value, 4)\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2614eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fisher's Exact Test Results:\n",
      "                         Risk                                  Challenge  \\\n",
      "0   Bias and fairness issues                          AI Not being used   \n",
      "1   Bias and fairness issues    Data privacy and cybersecurity concerns   \n",
      "2   Bias and fairness issues         Insufficient resources and funding   \n",
      "3   Bias and fairness issues       Interoperability with legacy systems   \n",
      "4   Bias and fairness issues  It's at the initial stage at Global level   \n",
      "5   Bias and fairness issues                  Lack of reliability on AI   \n",
      "6   Bias and fairness issues                     Lack of skilled talent   \n",
      "7   Bias and fairness issues                                         NA   \n",
      "8   Bias and fairness issues                             NOT APPLICABLE   \n",
      "9   Bias and fairness issues                                       None   \n",
      "10  Bias and fairness issues                             Not Applicable   \n",
      "11  Bias and fairness issues                       Not evaluated as yet   \n",
      "12  Bias and fairness issues    Not yet adapted but under consideration   \n",
      "13  Bias and fairness issues                Rapid technological changes   \n",
      "14  Bias and fairness issues                Still to determine use case   \n",
      "\n",
      "    OddsRatio  p_value  \n",
      "0        0.00   1.0000  \n",
      "1        1.76   0.3699  \n",
      "2        0.94   1.0000  \n",
      "3        3.70   0.0471  \n",
      "4        0.00   1.0000  \n",
      "5        0.00   1.0000  \n",
      "6        1.15   1.0000  \n",
      "7        0.00   0.5650  \n",
      "8        0.00   1.0000  \n",
      "9         inf   0.2295  \n",
      "10       3.54   0.4093  \n",
      "11       0.00   1.0000  \n",
      "12       0.00   1.0000  \n",
      "13       0.89   1.0000  \n",
      "14       0.00   1.0000  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nFisher's Exact Test Results:\\n\", results_df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99f4d9",
   "metadata": {},
   "source": [
    "# Reseach Question : Analyze the Impact of Governance Frameworks on Audit Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3b72cc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['What governance frameworks has your organization implemented? (Select all that apply)', 'Does your organization have established AI audit processes?'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mdata_01\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat governance frameworks has your organization implemented? (Select all that apply)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDoes your organization have established AI audit processes?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VSCodeProgram\\August22_2025 Statistical Report Objectives\\report\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VSCodeProgram\\August22_2025 Statistical Report Objectives\\report\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VSCodeProgram\\August22_2025 Statistical Report Objectives\\report\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['What governance frameworks has your organization implemented? (Select all that apply)', 'Does your organization have established AI audit processes?'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "df = data_01[[\n",
    "    \"What governance frameworks has your organization implemented? (Select all that apply)\",\n",
    "    \"Does your organization have established AI audit processes?\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6bf01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "report",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
